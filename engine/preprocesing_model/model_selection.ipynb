{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#from nltk import *\n",
    "import json\n",
    "\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "#import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Ejemplo con Stack de RNNs\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score #son metricas - area debajo de la curva roc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Ejemplo con Stack de RNNs\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- --------\n",
      "absl-py                      2.1.0\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "cachetools                   5.3.2\n",
      "certifi                      2024.2.2\n",
      "charset-normalizer           3.3.2\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.1\n",
      "debugpy                      1.8.1\n",
      "decorator                    5.1.1\n",
      "executing                    2.0.1\n",
      "flatbuffers                  23.5.26\n",
      "gast                         0.4.0\n",
      "google-auth                  2.28.1\n",
      "google-auth-oauthlib         1.0.0\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.62.0\n",
      "h5py                         3.10.0\n",
      "idna                         3.6\n",
      "ipykernel                    6.29.2\n",
      "ipython                      8.22.1\n",
      "jax                          0.4.24\n",
      "jedi                         0.19.1\n",
      "jupyter_client               8.6.0\n",
      "jupyter_core                 5.7.1\n",
      "keras                        2.12.0\n",
      "libclang                     16.0.6\n",
      "Markdown                     3.5.2\n",
      "MarkupSafe                   2.1.5\n",
      "matplotlib-inline            0.1.6\n",
      "ml-dtypes                    0.3.2\n",
      "nest-asyncio                 1.6.0\n",
      "numpy                        1.23.5\n",
      "oauthlib                     3.2.2\n",
      "opt-einsum                   3.3.0\n",
      "packaging                    23.2\n",
      "parso                        0.8.3\n",
      "pip                          24.0\n",
      "platformdirs                 4.2.0\n",
      "prompt-toolkit               3.0.43\n",
      "protobuf                     4.25.3\n",
      "psutil                       5.9.8\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.1\n",
      "pyasn1-modules               0.3.0\n",
      "Pygments                     2.17.2\n",
      "python-dateutil              2.8.2\n",
      "pywin32                      306\n",
      "pyzmq                        25.1.2\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "rsa                          4.9\n",
      "scipy                        1.12.0\n",
      "setuptools                   65.5.0\n",
      "six                          1.16.0\n",
      "stack-data                   0.6.3\n",
      "tensorboard                  2.12.3\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorflow-estimator         2.12.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.4.0\n",
      "tornado                      6.4\n",
      "traitlets                    5.14.1\n",
      "typing_extensions            4.9.0\n",
      "urllib3                      2.2.1\n",
      "wcwidth                      0.2.13\n",
      "Werkzeug                     3.0.1\n",
      "wheel                        0.42.0\n",
      "wrapt                        1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del archivo de tweets filtrados y trasformados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dev_esA = pd.read_csv(\"tweets_search_etiquetas_clean_test.csv\", sep = \";\", encoding = \"latin-1\")\n",
    "corpus_dev_esA.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener los vectores de entrada X y de salida Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idA = corpus_train_esA[corpus_train_esA.columns[0]]\n",
    "X_train_textA = corpus_train_esA[corpus_train_esA.columns[3]].fillna(' ')\n",
    "y_train_hsA = corpus_train_esA[[corpus_train_esA.columns[5], corpus_train_esA.columns[6], corpus_train_esA.columns[7]]]\n",
    "#y_train_hsA = corpus_train_esA[corpus_train_esA.columns[8]]\n",
    "y_train_hsA = y_train_hsA.values\n",
    "\n",
    "test_idA = corpus_dev_esA[corpus_train_esA.columns[0]]\n",
    "X_test_textA = corpus_dev_esA[corpus_dev_esA.columns[3]].fillna(' ')\n",
    "y_test_hsA = corpus_dev_esA[[corpus_dev_esA.columns[5], corpus_dev_esA.columns[6], corpus_dev_esA.columns[7]]]\n",
    "#y_test_hsA = corpus_dev_esA[corpus_dev_esA.columns[8]]\n",
    "y_test_hsA = y_test_hsA.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de los vectores numericos a partir del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(\n",
    "    # lowercase=True,\n",
    "    #stop_words=[word.decode('utf-8') for word in nltk.corpus.stopwords.words('spanish')],\n",
    "    #token_pattern=r'\\b\\w+\\b', #selects tokens of 2 or more alphanumeric characters \n",
    "    ngram_range=(3,5),#n-grams de palabras n = 1 a n = 3 (unigramas, bigramas y trigramas)\n",
    "    min_df=3,#ignorando los términos que tienen una frecuencia de documento estrictamente inferior a 5\n",
    ").fit(X_train_textA)\n",
    "\n",
    "X_train_cvectorized = cvectorizer.transform(X_train_textA).toarray()\n",
    "print(X_train_cvectorized.shape)\n",
    "\n",
    "X_test_cvectorized = cvectorizer.transform(X_test_textA).toarray()\n",
    "print(X_test_cvectorized.shape)\n",
    "\n",
    "dump(cvectorizer, 'count_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cvectorizer_vocabulary_.json\", mode = \"w\", encoding = \"latin-1\", ) as f:\n",
    "    f.write(json.dumps(cvectorizer.vocabulary_, indent = 4, ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper parametros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features =  10000 #len(cvectorizer.vocabulary_) or X_train_cvectorized.shape[1] #10000  # tamaño del diccionario de palabras comunes\n",
    "                      # (número de palabras a utilizar)\n",
    "maxlen = X_test_cvectorized.shape[1] #1775         # longitud máxima de cada secuencia \n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re neuronal con embedding y capas simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "# Capa embedding\n",
    "# input_dim : tamaño del vocabulario\n",
    "# output_dim: dimensión del vector al que se mapea\n",
    "model.add(Embedding(input_dim = max_features,  output_dim=32, input_length = maxlen)) # output_dim = 32\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.compile(optimizer='adam', loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "history_stackRNN = model.fit(\n",
    "    X_train_cvectorized, y_train_hsA,\n",
    "    epochs=15,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "print('Tiempo de entrenamiento:', time.time()-tic)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train_cvectorized, y_train_hsA, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "#plot_history(history)\n",
    "\n",
    "# make predictions\n",
    "testPredict_stackRNN = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_stackRNN.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red neuronal con embedding y redes densas con activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = batch_size # 50 #batch_size\n",
    "#maxlen = 15\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_features, \n",
    "                           output_dim = embedding_dim, \n",
    "                           input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_cvectorized, y_train_hsA,\n",
    "                    epochs=15,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test_cvectorized, y_test_hsA),\n",
    "                    batch_size=batch_size)\n",
    "                    \n",
    "loss, accuracy = model.evaluate(X_train_cvectorized, y_train_hsA, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "#plot_history(history)\n",
    "\n",
    "# make predictions\n",
    "testPredict_stackRNN = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_stackRNN.round()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### red neuronal con embedding y long short term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_out = 98\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_features, output_dim = embedding_dim, input_length = maxlen))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(X_train_cvectorized, y_train_hsA, epochs = 10, batch_size=batch_size, verbose = 2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train_cvectorized, y_train_hsA, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "#plot_history(history)\n",
    "\n",
    "# make predictions\n",
    "testPredict_stackRNN = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_stackRNN.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best_keras_network_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "input_list =  list(map(lambda x: str(x), y_train_hsA.round()))\n",
    "input_list =  list(map(lambda x: str(x), y_test_hsA.round()))\n",
    "input_list =  list(map(lambda x: str(x), testPredict_stackRNN.round()))\n",
    "c = Counter( input_list )\n",
    "\n",
    "print( c.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dev_esA[\"predict_test\"] = list(map(lambda x: str(x), testPredict_stackRNN.round()))\n",
    "\n",
    "corpus_dev_esA.to_csv(\"tweets_search_etiquetas_clean_test_predict.csv\", sep=\"\\t\", encoding = \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from api_twitter import StreammerTwitter\n",
    "\n",
    "streammer = StreammerTwitter()\n",
    "streammer.simulate_search_30(\"tweets_search_2.txt\", True, model_file = \"best_keras_network_model.h5\", cvectorizer_file = \"count_vectorizer.joblib\")\n",
    "\n",
    "#transformed_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
