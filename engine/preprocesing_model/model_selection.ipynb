{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import *\n",
    "import json\n",
    "\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Ejemplo con Stack de RNNs\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score #son metricas - area debajo de la curva roc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del archivo de tweets filtrados y trasformados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dev_esA = pd.read_csv(\"tweets_search_etiquetas_clean_test.csv\", sep = \";\", encoding = \"latin-1\")\n",
    "corpus_dev_esA.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener los vectores de entrada X y de salida Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idA = corpus_train_esA[corpus_train_esA.columns[0]]\n",
    "X_train_textA = corpus_train_esA[corpus_train_esA.columns[3]].fillna(' ')\n",
    "y_train_hsA = corpus_train_esA[[corpus_train_esA.columns[5], corpus_train_esA.columns[6], corpus_train_esA.columns[7]]]\n",
    "#y_train_hsA = corpus_train_esA[corpus_train_esA.columns[8]]\n",
    "y_train_hsA = y_train_hsA.values\n",
    "\n",
    "test_idA = corpus_dev_esA[corpus_train_esA.columns[0]]\n",
    "X_test_textA = corpus_dev_esA[corpus_dev_esA.columns[3]].fillna(' ')\n",
    "y_test_hsA = corpus_dev_esA[[corpus_dev_esA.columns[5], corpus_dev_esA.columns[6], corpus_dev_esA.columns[7]]]\n",
    "#y_test_hsA = corpus_dev_esA[corpus_dev_esA.columns[8]]\n",
    "y_test_hsA = y_test_hsA.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de los vectores numericos a partir del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(\n",
    "    # lowercase=True,\n",
    "    #stop_words=[word.decode('utf-8') for word in nltk.corpus.stopwords.words('spanish')],\n",
    "    #token_pattern=r'\\b\\w+\\b', #selects tokens of 2 or more alphanumeric characters \n",
    "    ngram_range=(3,5),#n-grams de palabras n = 1 a n = 3 (unigramas, bigramas y trigramas)\n",
    "    min_df=3,#ignorando los términos que tienen una frecuencia de documento estrictamente inferior a 5\n",
    ").fit(X_train_textA)\n",
    "\n",
    "X_train_cvectorized = cvectorizer.transform(X_train_textA).toarray()\n",
    "print(X_train_cvectorized.shape)\n",
    "\n",
    "X_test_cvectorized = cvectorizer.transform(X_test_textA).toarray()\n",
    "print(X_test_cvectorized.shape)\n",
    "\n",
    "dump(cvectorizer, 'count_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cvectorizer_vocabulary_.json\", mode = \"w\", encoding = \"latin-1\", ) as f:\n",
    "    f.write(json.dumps(cvectorizer.vocabulary_, indent = 4, ensure_ascii = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper parametros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features =  10000 #len(cvectorizer.vocabulary_) or X_train_cvectorized.shape[1] #10000  # tamaño del diccionario de palabras comunes\n",
    "                      # (número de palabras a utilizar)\n",
    "maxlen = X_test_cvectorized.shape[1] #1775         # longitud máxima de cada secuencia \n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re neuronal con embedding y capas simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "# Capa embedding\n",
    "# input_dim : tamaño del vocabulario\n",
    "# output_dim: dimensión del vector al que se mapea\n",
    "model.add(Embedding(input_dim = max_features,  output_dim=32, input_length = maxlen)) # output_dim = 32\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.compile(optimizer='adam', loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "history_stackRNN = model.fit(\n",
    "    X_train_cvectorized, y_train_hsA,\n",
    "    epochs=15,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "print('Tiempo de entrenamiento:', time.time()-tic)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train_cvectorized, y_train_hsA, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "#plot_history(history)\n",
    "\n",
    "# make predictions\n",
    "testPredict_stackRNN = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_stackRNN.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red neuronal con embedding y redes densas con activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = batch_size # 50 #batch_size\n",
    "#maxlen = 15\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_features, \n",
    "                           output_dim = embedding_dim, \n",
    "                           input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_cvectorized, y_train_hsA,\n",
    "                    epochs=15,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test_cvectorized, y_test_hsA),\n",
    "                    batch_size=batch_size)\n",
    "                    \n",
    "loss, accuracy = model.evaluate(X_train_cvectorized, y_train_hsA, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "#plot_history(history)\n",
    "\n",
    "# make predictions\n",
    "testPredict_stackRNN = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_stackRNN.round()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### red neuronal con embedding y long short term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_out = 98\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_features, output_dim = embedding_dim, input_length = maxlen))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(X_train_cvectorized, y_train_hsA, epochs = 10, batch_size=batch_size, verbose = 2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train_cvectorized, y_train_hsA, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_cvectorized, y_test_hsA, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "#plot_history(history)\n",
    "\n",
    "# make predictions\n",
    "testPredict_stackRNN = model.predict(X_test_cvectorized)\n",
    "print('\\t', 'Accuracy', accuracy_score(y_test_hsA, testPredict_stackRNN.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best_keras_network_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "input_list =  list(map(lambda x: str(x), y_train_hsA.round()))\n",
    "input_list =  list(map(lambda x: str(x), y_test_hsA.round()))\n",
    "input_list =  list(map(lambda x: str(x), testPredict_stackRNN.round()))\n",
    "c = Counter( input_list )\n",
    "\n",
    "print( c.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dev_esA[\"predict_test\"] = list(map(lambda x: str(x), testPredict_stackRNN.round()))\n",
    "\n",
    "corpus_dev_esA.to_csv(\"tweets_search_etiquetas_clean_test_predict.csv\", sep=\"\\t\", encoding = \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from api_twitter import StreammerTwitter\n",
    "\n",
    "streammer = StreammerTwitter()\n",
    "streammer.simulate_search_30(\"tweets_search_2.txt\", True, model_file = \"best_keras_network_model.h5\", cvectorizer_file = \"count_vectorizer.joblib\")\n",
    "\n",
    "#transformed_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
